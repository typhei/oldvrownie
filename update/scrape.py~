# -*- coding:utf-8 -*-
import urllib2
import time
import os
from bs4 import BeautifulSoup
from PIL import Image
import unicodedata
import math
import sqlite3
path = "../app/assets/images/"
imagetype = ["png", "gif", "jpg", "jpeg"]
NUM = 10

def download_img(src,maxsize,current):
    try:
        #URLから画像を取得
        filename ="unknown"
        i = urllib2.urlopen(src)
        filename = src.split("/")[-1]

        if os.path.isfile(path+filename):
            return "",""

        if filename.split(".")[-1] not in imagetype:
            raise "Unknown image type Error"

        if os.path.isfile(path+filename) != True:
            #saveDataというバイナリデータを作成
            saveData = open(path + filename, "wb")

            #saveDataに取得した画像を書き込み
            saveData.write(i.read())
            saveData.close()

            print ">>>get:",filename

        kaizoudo = Image.open(path+filename).size
        newsize = kaizoudo[0]*kaizoudo[1]

        if newsize > maxsize:
            if current != "":
                os.remove(path+current)
                print "-----removed:",current, "-----"
            return filename, newsize
        else:
            return "",""
        


        return filename

    #ダウンロードできなかった場合は空文字列を返す
    except Exception as e:
        print e
        print ">>>error:", src
        return ""




def main():
    
    #選別したURLの書き込み先ファイル
    f = open("sorted_URL")
    #railsのデータベース書き込み用ファイル
    wt = open("../db/seeds.rb", "w")
    wt.write("#coding: utf-8\n")

    count = 1

    
    c = sqlite3.connect("../db/development.sqlite3").cursor()
    db_data = list(c.execute("select url from pages;"))
    db_data = [x[0] for x in db_data]
    maxnum = list(c.execute("select max(number) from pages;"))[0][0]
    if maxnum is not None:
        maxnum += 10
    else:
        maxnum = 10
    print maxnum
    
    for url in f:
        try:
            if count > NUM:
                break
            #ページを読み込み．テキスト部分とimgタグを収集
            print url
            res = urllib2.urlopen(url)
            html = res.read()
            soup = BeautifulSoup(html)
            meta = soup.find_all("meta")
            title,description,image = "","",""
            image_file = ""
            maxsz = 0
            current = ""
            image = ""
            
            for m in meta:
                if m.get("property") == "og:title":
                    title = m.get("content")
                elif m.get("property") == "og:description":
                    description = m.get("content")
                elif m.get("property") == "og:image":
                    image = m.get("content")
                    print "image:",image
                    result,imagesize = download_img(image,maxsz,image_file)
                    if result != "":
                        image_file = result
                    if imagesize != "":
                        maxsz = imagesize


            
            if url.replace("\n","") in db_data:
                print "continue"
                continue
            
            
            #データベース書き込み用ファイルにページ情報を書き込む
            str1 = "@page.title = \"" + title.encode("utf-8") + "\""
            str1 = str1.replace("\n", "")
            str1 += "\n"
            urls = url.replace("\n", "")
            print url
            wt.write("@page = Page.new\n")
            wt.write("@page.number = " + str(maxnum) + "\n")
            wt.write(str1)
            wt.write("@page.url = \"" + urls + "\"\n")
            wt.write("@page.image = \"/assets/" + image_file + "\"\n")
            wt.write("@page.body = \"" + description.encode("utf-8") + "\"\n")
            wt.write("@page.save\n\n")
            count += 1
            maxnum -= 1
            time.sleep(1)
            

        #エラー処理
        except Exception as e:
            print e

    f.close()
    wt.close()

if __name__ == "__main__":
    main()
